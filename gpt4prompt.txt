I have four files of python code as below, I need to refactor the code to be useful for other prompt flows, let's start by adding a flow for the user to enter a topic and find the most important expert on that topic, then find that experts most influential ideas:

config.py:
`import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Set up the OpenAI API key
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
`

llm_chains.py:
`from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.chains.base import Chain
from typing import Dict, List
from prompt_templates import prompt_templates, dependencies
import config

# Create an OpenAI instance with a high temperature for more randomness
llm = OpenAI(temperature=0.9)

# Create LLM chains for each prompt template
llm_chains = {key: LLMChain(llm=llm, prompt=prompt_templates[key]) for key in prompt_templates}

class CustomSequentialChain(Chain):
    llm_chains: Dict[str, LLMChain]

    @property
    def input_keys(self) -> List[str]:
        return ['product']

    @property
    def output_keys(self) -> List[str]:
        return list(prompt_templates.keys())

    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:
        results = {}
        visited = set()

        def process_chain(key: str):
            if key not in visited:
                visited.add(key)
                for dependency in dependencies[key]:
                    process_chain(dependency)
                required_inputs = {k: inputs[k] for k in self.llm_chains[key].prompt.input_variables if k in inputs}
                output = self.llm_chains[key].run(required_inputs)
                results[key] = output
                inputs[key] = output

        for key in self.llm_chains:
            process_chain(key)

        return results

    def run_chain(self, inputs: Dict[str, str]) -> Dict[str, str]:
        return self(inputs)
`

prompt_templates.py:
`from langchain.prompts import PromptTemplate

# Define a library of prompt templates
prompt_templates = {
    "company_name": PromptTemplate(
        input_variables=["product"],
        template="What is a good name for a company that makes {product}?",
    ),
    "slogan": PromptTemplate(
        input_variables=["company_name"],
        template="What is a good slogan for {company_name}?",
    ),
    "marketing_strategy": PromptTemplate(
        input_variables=["slogan", "company_name"],
        template="What is a good marketing strategy for {company_name} with the slogan '{slogan}'?",
    ),
}

# Define the dependencies between templates
dependencies = {
    "company_name": [],
    "slogan": ["company_name"],
    "marketing_strategy": ["slogan", "company_name"],
}

`

main.py:
`from llm_chains import CustomSequentialChain, llm_chains

# Create the CustomSequentialChain
overall_chain = CustomSequentialChain(llm_chains=llm_chains)

# Get product input from the user
product = input("Enter a product: ")

# Run the chains
result = overall_chain.run_chain({"product": product})

# Print the output
for key in overall_chain.output_keys:
    print(f"{key.capitalize()}: {result[key]}")
`